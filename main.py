from fastapi import FastAPI, Request, Query, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from real_crawler import RealNewsCrawler
import uvicorn
from typing import List, Dict, Optional
import logging
from datetime import datetime, timedelta
import json
import os
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import pytz

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="ê´€ì‹¬ì‚¬ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬")

# í…œí”Œë¦¿ ì„¤ì •
templates = Jinja2Templates(directory="templates")

# í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
news_crawler = RealNewsCrawler()

# ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# í•œêµ­ ì‹œê°„ëŒ€
KST = pytz.timezone('Asia/Seoul')

# ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
scheduler = BackgroundScheduler(timezone=KST)


def get_cache_file_path(category: str) -> str:
    """ì¹´í…Œê³ ë¦¬ë³„ ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
    return os.path.join(DATA_DIR, f"{category}.json")


def save_news_to_file(category: str, articles: List[Dict]):
    """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        file_path = get_cache_file_path(category)
        data = {
            'articles': articles,
            'cached_at': datetime.now(KST).isoformat(),
            'category': category,
            'category_name': news_crawler.CATEGORIES.get(category, {}).get('name', category)
        }
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"[íŒŒì¼ ì €ì¥] {category}: {len(articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"[íŒŒì¼ ì €ì¥] {category} ì˜¤ë¥˜: {e}", exc_info=True)


def load_news_from_file(category: str) -> Optional[Dict]:
    """íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ"""
    try:
        file_path = get_cache_file_path(category)
        if not os.path.exists(file_path):
            return None
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"[íŒŒì¼ ë¡œë“œ] {category}: {len(data.get('articles', []))}ê°œ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ")
        return data
    except Exception as e:
        logger.error(f"[íŒŒì¼ ë¡œë“œ] {category} ì˜¤ë¥˜: {e}", exc_info=True)
        return None


def crawl_all_categories():
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (ìŠ¤ì¼€ì¤„ë§ìš©)"""
    logger.info("=" * 60)
    logger.info("[ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì‹œì‘] ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘")
    logger.info(f"[ì‹œê°„] {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')}")
    
    categories = list(news_crawler.CATEGORIES.keys())
    success_count = 0
    fail_count = 0
    
    for category in categories:
        try:
            logger.info(f"[í¬ë¡¤ë§] {category} ì‹œì‘...")
            articles = news_crawler.crawl_category(category)
            
            # í¬ë§·íŒ…
            formatted_articles = []
            for i, article in enumerate(articles):
                url = article.get('url', '')
                if 'example.com' in url or not url or url.startswith('https://example'):
                    continue
                
                formatted_articles.append({
                    "id": f"{category}_{i}",
                    "title": article.get('title', ''),
                    "url": url,
                    "source": article.get('source', ''),
                    "publishedAt": article.get('publishedAt', datetime.now(KST).isoformat()),
                    "imageUrl": article.get('imageUrl', '') or '',
                    "summary": article.get('summary', '') or ''
                })
            
            # íŒŒì¼ ì €ì¥
            save_news_to_file(category, formatted_articles)
            success_count += 1
            logger.info(f"[í¬ë¡¤ë§ ì™„ë£Œ] {category}: {len(formatted_articles)}ê°œ ê¸°ì‚¬")
            
        except Exception as e:
            fail_count += 1
            logger.error(f"[í¬ë¡¤ë§ ì‹¤íŒ¨] {category}: {e}", exc_info=True)
    
    logger.info(f"[ìŠ¤ì¼€ì¤„ í¬ë¡¤ë§ ì™„ë£Œ] ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {fail_count}ê°œ")
    logger.info("=" * 60)


@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    logger.info("ì•± ì‹œì‘ - ìŠ¤ì¼€ì¤„ë§ í¬ë¡¤ëŸ¬ ì¤€ë¹„ ì¤‘...")
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •: ë§¤ì¼ 8ì‹œ, 17ì‹œì— í¬ë¡¤ë§
    scheduler.add_job(
        crawl_all_categories,
        trigger=CronTrigger(hour='8,17', minute=0),  # ë§¤ì¼ 8ì‹œ, 17ì‹œ
        id='daily_crawl',
        name='ë§¤ì¼ ì•„ì¹¨ 8ì‹œ, ì €ë… 5ì‹œ í¬ë¡¤ë§',
        replace_existing=True
    )
    
    scheduler.start()
    logger.info("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì™„ë£Œ (ë§¤ì¼ 8ì‹œ, 17ì‹œ ìë™ í¬ë¡¤ë§)")
    
    # ì•± ì‹œì‘ ì‹œ ì¦‰ì‹œ í•œ ë²ˆ í¬ë¡¤ë§ (ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°)
    logger.info("[ì´ˆê¸° í¬ë¡¤ë§] ì €ì¥ëœ ë°ì´í„° í™•ì¸ ì¤‘...")
    has_data = False
    for category in news_crawler.CATEGORIES.keys():
        if load_news_from_file(category):
            has_data = True
            break
    
    if not has_data:
        logger.info("[ì´ˆê¸° í¬ë¡¤ë§] ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ì–´ ì¦‰ì‹œ í¬ë¡¤ë§ ì‹œì‘...")
        crawl_all_categories()
    else:
        logger.info("[ì´ˆê¸° í¬ë¡¤ë§] ì €ì¥ëœ ë°ì´í„°ê°€ ìˆì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤.")


@app.on_event("shutdown")
async def shutdown_event():
    """ì•± ì¢…ë£Œ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ"""
    scheduler.shutdown()
    logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì™„ë£Œ")


@app.get("/", response_class=HTMLResponse)
async def home(request: Request, category: Optional[str] = Query(None)):
    """ë©”ì¸ í˜ì´ì§€"""
    categories = news_crawler.get_all_categories()
    
    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "categories": categories,
            "current_category": category
        }
    )


@app.get("/api/categories")
async def get_categories():
    """ì¹´í…Œê³ ë¦¬ ëª©ë¡ API"""
    categories = news_crawler.get_all_categories()
    return {
        "success": True,
        "categories": categories
    }


@app.get("/api/news")
async def get_news(category: Optional[str] = Query(None)):
    """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ API (íŒŒì¼ì—ì„œ ì¦‰ì‹œ ë°˜í™˜)"""
    if not category:
        return JSONResponse(
            status_code=400,
            content={
                "success": False,
                "message": "ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.",
                "categories": news_crawler.get_all_categories()
            }
        )
    
    if category not in news_crawler.CATEGORIES:
        return JSONResponse(
            status_code=404,
            content={
                "success": False,
                "message": f"ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {category}"
            }
        )
    
    # íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (ì¦‰ì‹œ ë°˜í™˜)
    cached_data = load_news_from_file(category)
    
    if cached_data and cached_data.get('articles'):
        logger.info(f"[API] {category} ì¹´í…Œê³ ë¦¬ íŒŒì¼ì—ì„œ ë¡œë“œ: {len(cached_data['articles'])}ê°œ")
        return {
            "success": True,
            "category": category,
            "category_name": cached_data.get('category_name', news_crawler.CATEGORIES[category]['name']),
            "count": len(cached_data['articles']),
            "articles": cached_data['articles'],
            "cached_at": cached_data.get('cached_at')
        }
    else:
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ ë°˜í™˜
        logger.warning(f"[API] {category} ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì—†ìŒ")
        return {
            "success": True,
            "category": category,
            "category_name": news_crawler.CATEGORIES[category]['name'],
            "count": 0,
            "articles": [],
            "message": "ì•„ì§ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ í¬ë¡¤ë§ ì‹œê°„(ì˜¤ì „ 8ì‹œ ë˜ëŠ” ì˜¤í›„ 5ì‹œ)ì„ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."
        }


@app.get("/api/news/refresh")
async def refresh_news(category: Optional[str] = Query(None)):
    """ë‰´ìŠ¤ ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ (ì¦‰ì‹œ í¬ë¡¤ë§)"""
    if not category:
        return JSONResponse(
            status_code=400,
            content={
                "success": False,
                "message": "ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
            }
        )
    
    if category not in news_crawler.CATEGORIES:
        return JSONResponse(
            status_code=404,
            content={
                "success": False,
                "message": f"ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {category}"
            }
        )
    
    try:
        logger.info(f"[ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨] {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘...")
        
        # í¬ë¡¤ë§ ìˆ˜í–‰
        articles = news_crawler.crawl_category(category)
        
        # í¬ë§·íŒ…
        formatted_articles = []
        for i, article in enumerate(articles):
            url = article.get('url', '')
            if 'example.com' in url or not url or url.startswith('https://example'):
                continue
            
            formatted_articles.append({
                "id": f"{category}_{i}",
                "title": article.get('title', ''),
                "url": url,
                "source": article.get('source', ''),
                "publishedAt": article.get('publishedAt', datetime.now(KST).isoformat()),
                "imageUrl": article.get('imageUrl', '') or '',
                "summary": article.get('summary', '') or ''
            })
        
        # íŒŒì¼ ì €ì¥
        save_news_to_file(category, formatted_articles)
        
        return {
            "success": True,
            "message": "ë‰´ìŠ¤ê°€ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "category": category,
            "category_name": news_crawler.CATEGORIES[category]['name'],
            "count": len(formatted_articles)
        }
    
    except Exception as e:
        logger.error(f"[ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨] {category} ì˜¤ë¥˜: {e}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"ìƒˆë¡œê³ ì¹¨ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
        )


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    cached_categories = []
    total_news = 0
    
    for category in news_crawler.CATEGORIES.keys():
        data = load_news_from_file(category)
        if data and data.get('articles'):
            cached_categories.append(category)
            total_news += len(data['articles'])
    
    # ë‹¤ìŒ í¬ë¡¤ë§ ì‹œê°„ ê³„ì‚°
    now = datetime.now(KST)
    next_crawl_times = []
    
    # ì˜¤ëŠ˜ 8ì‹œ, 17ì‹œ í™•ì¸
    today_8am = now.replace(hour=8, minute=0, second=0, microsecond=0)
    today_5pm = now.replace(hour=17, minute=0, second=0, microsecond=0)
    
    if now < today_8am:
        next_crawl_times.append(today_8am.isoformat())
    elif now < today_5pm:
        next_crawl_times.append(today_5pm.isoformat())
    else:
        # ë‚´ì¼ 8ì‹œ
        tomorrow_8am = (now + timedelta(days=1)).replace(hour=8, minute=0, second=0, microsecond=0)
        next_crawl_times.append(tomorrow_8am.isoformat())
    
    return {
        "status": "healthy",
        "cached_categories": cached_categories,
        "total_news_count": total_news,
        "next_crawl_times": next_crawl_times,
        "scheduler_running": scheduler.running
    }


if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ¯ ê´€ì‹¬ì‚¬ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‹œì‘ (ìŠ¤ì¼€ì¤„ë§ ëª¨ë“œ)")
    print("="*60)
    print("\nğŸ“ ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ë‹¤ìŒ ì£¼ì†Œë¡œ ì ‘ì†í•˜ì„¸ìš”:")
    print("   ğŸ‘‰ http://localhost:8000")
    print("\nâ° ìë™ í¬ë¡¤ë§ ì‹œê°„:")
    print("   - ë§¤ì¼ ì˜¤ì „ 8ì‹œ")
    print("   - ë§¤ì¼ ì˜¤í›„ 5ì‹œ")
    print("\nğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸:")
    print("   - GET /api/categories     : ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ")
    print("   - GET /api/news?category= : ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ì¡°íšŒ (ì¦‰ì‹œ ë°˜í™˜)")
    print("   - GET /api/news/refresh?category= : ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨")
    print("   - GET /health             : ì„œë²„ ìƒíƒœ í™•ì¸")
    print("\nğŸ“‚ ì§€ì› ì¹´í…Œê³ ë¦¬:")
    categories = RealNewsCrawler().get_all_categories()
    for key, name in categories.items():
        print(f"   - {name} ({key})")
    print("\nâš ï¸  ë„¤ì´ë²„ API ì‚¬ìš© ì‹œ í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”:")
    print("   export NAVER_CLIENT_ID='your_client_id'")
    print("   export NAVER_CLIENT_SECRET='your_client_secret'")
    print("\nğŸ’¾ ë°ì´í„° ì €ì¥ ìœ„ì¹˜: data/ ë””ë ‰í† ë¦¬")
    print("\nâ¹ï¸  ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”\n")
    print("="*60 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
